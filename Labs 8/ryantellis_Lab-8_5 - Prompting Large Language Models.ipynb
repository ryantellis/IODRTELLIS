{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {
    "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {
    "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612"
   },
   "source": [
    "# Lab 8.5 - Prompting Large Language Models\n",
    "\n",
    "In this lab we will practise prompting with a few Large Language Models (LLMs) using Groq (not to be confused with Grok). Groq is a platform that provides access to their custom-built AI hardware via APIs, allowing users to run open-source models such as Llama.\n",
    "\n",
    "We shall see that while LLMs are powerful tools, how you ask a question or frame a task can dramatically influence the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {
    "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {
    "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719"
   },
   "source": [
    "Step 1: Sign up for a free Groq account at https://console.groq.com/home .\n",
    "\n",
    "Step 2: Create a new API key at https://console.groq.com/keys. Copy-paste it into an empty text file called 'groq_key.txt'.\n",
    "\n",
    "Running the next cell will then read in this key and assign it to the variable `groq_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {
    "id": "56441b82-ddc5-46e9-9583-878e8a807b7f"
   },
   "outputs": [],
   "source": [
    "groqfilename = r'groq_key.txt' # this file contains a single line containing your Groq API key only\n",
    "try:\n",
    "    with open(groqfilename, 'r') as f:\n",
    "        groq_key = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88",
   "metadata": {
    "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.28.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\ryant\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Downloading groq-0.28.0-py3-none-any.whl (130 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.28.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c",
   "metadata": {
    "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227",
   "metadata": {
    "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227"
   },
   "source": [
    "First create an instance of the Groq client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b",
   "metadata": {
    "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<groq.Groq at 0x22f74556450>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Groq(api_key=groq_key)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097cd77-7a8d-4502-ae6e-607782706b26",
   "metadata": {
    "id": "a097cd77-7a8d-4502-ae6e-607782706b26"
   },
   "source": [
    "The following code shows what models are currently accessible through Groq. `context_window` refers to the size of memory (in tokens) during a session and `max_completion_tokens` is the maximum number of tokens that are generated in an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4",
   "metadata": {
    "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>active</th>\n",
       "      <th>context_window</th>\n",
       "      <th>public_apps</th>\n",
       "      <th>max_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-86m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632165</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-22m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632101</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qwen/qwen3-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1748396646</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>40960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta-llama/llama-guard-4-12b</td>\n",
       "      <td>model</td>\n",
       "      <td>1746743847</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743877158</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743874824</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>compound-beta-mini</td>\n",
       "      <td>model</td>\n",
       "      <td>1742953279</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1741214760</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>compound-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1740880017</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>playai-tts-arabic</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682783</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>playai-tts</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682771</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mistral-saba-24b</td>\n",
       "      <td>model</td>\n",
       "      <td>1739996492</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>True</td>\n",
       "      <td>32768</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737924940</td>\n",
       "      <td>DeepSeek / Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allam-2-7b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737672203</td>\n",
       "      <td>SDAIA</td>\n",
       "      <td>True</td>\n",
       "      <td>4096</td>\n",
       "      <td>None</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>model</td>\n",
       "      <td>1733447754</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>whisper-large-v3-turbo</td>\n",
       "      <td>model</td>\n",
       "      <td>1728413088</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama3-70b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3-8b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>distil-whisper-large-v3-en</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Hugging Face</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>whisper-large-v3</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id object     created  \\\n",
       "16            meta-llama/llama-prompt-guard-2-86m  model  1748632165   \n",
       "12            meta-llama/llama-prompt-guard-2-22m  model  1748632101   \n",
       "5                                  qwen/qwen3-32b  model  1748396646   \n",
       "0                    meta-llama/llama-guard-4-12b  model  1746743847   \n",
       "19  meta-llama/llama-4-maverick-17b-128e-instruct  model  1743877158   \n",
       "6       meta-llama/llama-4-scout-17b-16e-instruct  model  1743874824   \n",
       "3                              compound-beta-mini  model  1742953279   \n",
       "14                                   qwen-qwq-32b  model  1741214760   \n",
       "17                                  compound-beta  model  1740880017   \n",
       "10                              playai-tts-arabic  model  1740682783   \n",
       "21                                     playai-tts  model  1740682771   \n",
       "13                               mistral-saba-24b  model  1739996492   \n",
       "18                  deepseek-r1-distill-llama-70b  model  1737924940   \n",
       "1                                      allam-2-7b  model  1737672203   \n",
       "7                         llama-3.3-70b-versatile  model  1733447754   \n",
       "9                          whisper-large-v3-turbo  model  1728413088   \n",
       "8                                    gemma2-9b-it  model  1693721698   \n",
       "15                                llama3-70b-8192  model  1693721698   \n",
       "4                            llama-3.1-8b-instant  model  1693721698   \n",
       "2                                  llama3-8b-8192  model  1693721698   \n",
       "20                     distil-whisper-large-v3-en  model  1693721698   \n",
       "11                               whisper-large-v3  model  1693721698   \n",
       "\n",
       "           owned_by  active  context_window public_apps  max_completion_tokens  \n",
       "16             Meta    True             512        None                    512  \n",
       "12             Meta    True             512        None                    512  \n",
       "5     Alibaba Cloud    True          131072        None                  40960  \n",
       "0              Meta    True          131072        None                   1024  \n",
       "19             Meta    True          131072        None                   8192  \n",
       "6              Meta    True          131072        None                   8192  \n",
       "3              Groq    True          131072        None                   8192  \n",
       "14    Alibaba Cloud    True          131072        None                 131072  \n",
       "17             Groq    True          131072        None                   8192  \n",
       "10           PlayAI    True            8192        None                   8192  \n",
       "21           PlayAI    True            8192        None                   8192  \n",
       "13       Mistral AI    True           32768        None                  32768  \n",
       "18  DeepSeek / Meta    True          131072        None                 131072  \n",
       "1             SDAIA    True            4096        None                   4096  \n",
       "7              Meta    True          131072        None                  32768  \n",
       "9            OpenAI    True             448        None                    448  \n",
       "8            Google    True            8192        None                   8192  \n",
       "15             Meta    True            8192        None                   8192  \n",
       "4              Meta    True          131072        None                 131072  \n",
       "2              Meta    True            8192        None                   8192  \n",
       "20     Hugging Face    True             448        None                    448  \n",
       "11           OpenAI    True             448        None                    448  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "pd.DataFrame(response.json()['data']).sort_values(['created'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438489-c102-434b-872d-e807169deef6",
   "metadata": {
    "id": "2b438489-c102-434b-872d-e807169deef6"
   },
   "source": [
    "The Groq client object enables interaction with the Groq REST API and a chat completion request is made via the client.chat.completions.create method.\n",
    "\n",
    "The most important arguments of the client.chat.completions.create method are the following:\n",
    "* messages: a list of messages (dictionary form) that make up the conversation to date\n",
    "* model: a string indicating which model to use (see [list of models](https://console.groq.com/docs/models))\n",
    "* max_completion_tokens: the maximum number of tokens that are generated in the chat completion\n",
    "* response_format: setting this to `{ \"type\": \"json_object\" }` enables JSON output\n",
    "* seed: sample deterministically as best as possible, though identical outputs each time are not guaranteed\n",
    "* temperature: between 0 and 2 where higher values like 0.8 make the output more random (creative) and values like 0.2 are more focused and deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99216392-acc3-4a00-826b-c166a1e52534",
   "metadata": {
    "id": "99216392-acc3-4a00-826b-c166a1e52534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module groq.resources.chat.completions:\n",
      "\n",
      "create(*, messages: 'Iterable[ChatCompletionMessageParam]', model: \"Union[str, Literal['gemma2-9b-it', 'llama-3.3-70b-versatile', 'llama-3.1-8b-instant', 'llama-guard-3-8b', 'llama3-70b-8192', 'llama3-8b-8192']]\", exclude_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, function_call: 'Optional[completion_create_params.FunctionCall] | NotGiven' = NOT_GIVEN, functions: 'Optional[Iterable[completion_create_params.Function]] | NotGiven' = NOT_GIVEN, include_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN, logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN, max_completion_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, metadata: 'Optional[Dict[str, str]] | NotGiven' = NOT_GIVEN, n: 'Optional[int] | NotGiven' = NOT_GIVEN, parallel_tool_calls: 'Optional[bool] | NotGiven' = NOT_GIVEN, presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, reasoning_effort: \"Optional[Literal['none', 'default']] | NotGiven\" = NOT_GIVEN, reasoning_format: \"Optional[Literal['hidden', 'raw', 'parsed']] | NotGiven\" = NOT_GIVEN, response_format: 'Optional[completion_create_params.ResponseFormat] | NotGiven' = NOT_GIVEN, search_settings: 'Optional[completion_create_params.SearchSettings] | NotGiven' = NOT_GIVEN, seed: 'Optional[int] | NotGiven' = NOT_GIVEN, service_tier: \"Optional[Literal['auto', 'on_demand', 'flex']] | NotGiven\" = NOT_GIVEN, stop: 'Union[Optional[str], List[str], None] | NotGiven' = NOT_GIVEN, store: 'Optional[bool] | NotGiven' = NOT_GIVEN, stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN, temperature: 'Optional[float] | NotGiven' = NOT_GIVEN, tool_choice: 'Optional[ChatCompletionToolChoiceOptionParam] | NotGiven' = NOT_GIVEN, tools: 'Optional[Iterable[ChatCompletionToolParam]] | NotGiven' = NOT_GIVEN, top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN, top_p: 'Optional[float] | NotGiven' = NOT_GIVEN, user: 'Optional[str] | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of groq.resources.chat.completions.Completions instance\n",
      "    Creates a model response for the given chat conversation.\n",
      "\n",
      "    Args:\n",
      "      messages: A list of messages comprising the conversation so far.\n",
      "\n",
      "      model: ID of the model to use. For details on which models are compatible with the Chat\n",
      "          API, see available [models](https://console.groq.com/docs/models)\n",
      "\n",
      "      exclude_domains: Deprecated: Use search_settings.exclude_domains instead. A list of domains to\n",
      "          exclude from the search results when the model uses a web search tool.\n",
      "\n",
      "      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n",
      "          existing frequency in the text so far, decreasing the model's likelihood to\n",
      "          repeat the same line verbatim.\n",
      "\n",
      "      function_call: Deprecated in favor of `tool_choice`.\n",
      "\n",
      "          Controls which (if any) function is called by the model. `none` means the model\n",
      "          will not call a function and instead generates a message. `auto` means the model\n",
      "          can pick between generating a message or calling a function. Specifying a\n",
      "          particular function via `{\"name\": \"my_function\"}` forces the model to call that\n",
      "          function.\n",
      "\n",
      "          `none` is the default when no functions are present. `auto` is the default if\n",
      "          functions are present.\n",
      "\n",
      "      functions: Deprecated in favor of `tools`.\n",
      "\n",
      "          A list of functions the model may generate JSON inputs for.\n",
      "\n",
      "      include_domains: Deprecated: Use search_settings.include_domains instead. A list of domains to\n",
      "          include in the search results when the model uses a web search tool.\n",
      "\n",
      "      logit_bias: This is not yet supported by any of our models. Modify the likelihood of\n",
      "          specified tokens appearing in the completion.\n",
      "\n",
      "      logprobs: This is not yet supported by any of our models. Whether to return log\n",
      "          probabilities of the output tokens or not. If true, returns the log\n",
      "          probabilities of each output token returned in the `content` of `message`.\n",
      "\n",
      "      max_completion_tokens: The maximum number of tokens that can be generated in the chat completion. The\n",
      "          total length of input tokens and generated tokens is limited by the model's\n",
      "          context length.\n",
      "\n",
      "      max_tokens: Deprecated in favor of `max_completion_tokens`. The maximum number of tokens\n",
      "          that can be generated in the chat completion. The total length of input tokens\n",
      "          and generated tokens is limited by the model's context length.\n",
      "\n",
      "      metadata: This parameter is not currently supported.\n",
      "\n",
      "      n: How many chat completion choices to generate for each input message. Note that\n",
      "          the current moment, only n=1 is supported. Other values will result in a 400\n",
      "          response.\n",
      "\n",
      "      parallel_tool_calls: Whether to enable parallel function calling during tool use.\n",
      "\n",
      "      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\n",
      "          whether they appear in the text so far, increasing the model's likelihood to\n",
      "          talk about new topics.\n",
      "\n",
      "      reasoning_effort: this field is only available for qwen3 models. Set to 'none' to disable\n",
      "          reasoning. Set to 'default' or null to let Qwen reason.\n",
      "\n",
      "      reasoning_format: Specifies how to output reasoning tokens\n",
      "\n",
      "      response_format: An object specifying the format that the model must output. Setting to\n",
      "          `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs\n",
      "          which ensures the model will match your supplied JSON schema. json_schema\n",
      "          response format is only supported on llama 4 models. Setting to\n",
      "          `{ \"type\": \"json_object\" }` enables the older JSON mode, which ensures the\n",
      "          message the model generates is valid JSON. Using `json_schema` is preferred for\n",
      "          models that support it.\n",
      "\n",
      "      search_settings: Settings for web search functionality when the model uses a web search tool.\n",
      "\n",
      "      seed: If specified, our system will make a best effort to sample deterministically,\n",
      "          such that repeated requests with the same `seed` and parameters should return\n",
      "          the same result. Determinism is not guaranteed, and you should refer to the\n",
      "          `system_fingerprint` response parameter to monitor changes in the backend.\n",
      "\n",
      "      service_tier: The service tier to use for the request. Defaults to `on_demand`.\n",
      "\n",
      "          - `auto` will automatically select the highest tier available within the rate\n",
      "            limits of your organization.\n",
      "          - `flex` uses the flex tier, which will succeed or fail quickly.\n",
      "\n",
      "      stop: Up to 4 sequences where the API will stop generating further tokens. The\n",
      "          returned text will not contain the stop sequence.\n",
      "\n",
      "      store: This parameter is not currently supported.\n",
      "\n",
      "      stream: If set, partial message deltas will be sent. Tokens will be sent as data-only\n",
      "          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n",
      "          as they become available, with the stream terminated by a `data: [DONE]`\n",
      "          message. [Example code](/docs/text-chat#streaming-a-chat-completion).\n",
      "\n",
      "      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n",
      "          make the output more random, while lower values like 0.2 will make it more\n",
      "          focused and deterministic. We generally recommend altering this or top_p but not\n",
      "          both.\n",
      "\n",
      "      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\n",
      "          not call any tool and instead generates a message. `auto` means the model can\n",
      "          pick between generating a message or calling one or more tools. `required` means\n",
      "          the model must call one or more tools. Specifying a particular tool via\n",
      "          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n",
      "          call that tool.\n",
      "\n",
      "          `none` is the default when no tools are present. `auto` is the default if tools\n",
      "          are present.\n",
      "\n",
      "      tools: A list of tools the model may call. Currently, only functions are supported as a\n",
      "          tool. Use this to provide a list of functions the model may generate JSON inputs\n",
      "          for. A max of 128 functions are supported.\n",
      "\n",
      "      top_logprobs: This is not yet supported by any of our models. An integer between 0 and 20\n",
      "          specifying the number of most likely tokens to return at each token position,\n",
      "          each with an associated log probability. `logprobs` must be set to `true` if\n",
      "          this parameter is used.\n",
      "\n",
      "      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\n",
      "          model considers the results of the tokens with top_p probability mass. So 0.1\n",
      "          means only the tokens comprising the top 10% probability mass are considered. We\n",
      "          generally recommend altering this or temperature but not both.\n",
      "\n",
      "      user: A unique identifier representing your end-user, which can help us monitor and\n",
      "          detect abuse.\n",
      "\n",
      "      extra_headers: Send extra headers\n",
      "\n",
      "      extra_query: Add additional query parameters to the request\n",
      "\n",
      "      extra_body: Add additional JSON properties to the request\n",
      "\n",
      "      timeout: Override the client-level default timeout for this request, in seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7",
   "metadata": {
    "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7"
   },
   "source": [
    "As a first example, note how the messages input is given as a list of a dictionaries with `role` and `content` keys. This is in a ChatML format recognised by many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81",
   "metadata": {
    "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models are artificial intelligence (AI) systems that process and generate human-like language. They work by:\n",
      "\n",
      "1. **Training on vast amounts of text data**: The model is trained on a massive dataset of text, which allows it to learn patterns, relationships, and structures of language.\n",
      "2. **Learning to predict the next word**: The model is trained to predict the next word in a sequence, given the context of the previous words. This is done using complex algorithms and neural networks.\n",
      "3. **Generating text based on context**: Once trained, the model can generate text by predicting the next word, given a prompt or context. This process is repeated to create coherent and natural-sounding text.\n",
      "\n",
      "The key technologies behind large language models include:\n",
      "\n",
      "* **Neural networks**: Complex algorithms that mimic the human brain's ability to learn and process information.\n",
      "* **Deep learning**: A subset of machine learning that uses neural networks to analyze and generate data.\n",
      "* **Transformer architecture**: A specific type of neural network architecture that is well-suited for natural language processing tasks.\n",
      "\n",
      "These models can perform a wide range of tasks, including language translation, text summarization, and conversation generation.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {   \"role\": \"system\", # sets the persona of the model\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": \"Explain briefly how large language models work\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b60c2-4300-4338-b325-c3ea876c1afe",
   "metadata": {
    "id": "728b60c2-4300-4338-b325-c3ea876c1afe"
   },
   "source": [
    "The output is in Markdown format so the following line formats this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88845bb3-4a3e-403a-93ab-439c46aa1832",
   "metadata": {
    "id": "88845bb3-4a3e-403a-93ab-439c46aa1832"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large language models are artificial intelligence (AI) systems that process and generate human-like language. They work by:\n",
       "\n",
       "1. **Training on vast amounts of text data**: The model is trained on a massive dataset of text, which allows it to learn patterns, relationships, and structures of language.\n",
       "2. **Learning to predict the next word**: The model is trained to predict the next word in a sequence, given the context of the previous words. This is done using complex algorithms and neural networks.\n",
       "3. **Generating text based on context**: Once trained, the model can generate text by predicting the next word, given a prompt or context. This process is repeated to create coherent and natural-sounding text.\n",
       "\n",
       "The key technologies behind large language models include:\n",
       "\n",
       "* **Neural networks**: Complex algorithms that mimic the human brain's ability to learn and process information.\n",
       "* **Deep learning**: A subset of machine learning that uses neural networks to analyze and generate data.\n",
       "* **Transformer architecture**: A specific type of neural network architecture that is well-suited for natural language processing tasks.\n",
       "\n",
       "These models can perform a wide range of tasks, including language translation, text summarization, and conversation generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {
    "id": "e1c8c97b-1711-4355-b108-86bed769c109"
   },
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {
    "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4"
   },
   "source": [
    "We start with a llama3-8b-8192, a model using just over 8 billion parameters with at most 8192 tokens produced as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {
    "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2"
   },
   "source": [
    "Here is an article to be summarised from the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {
    "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29"
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake Â» . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {
    "id": "9754257a-74ff-4747-a18e-7ebfbc82639c"
   },
   "source": [
    "**Exercise:**\n",
    "Summarise the story text using the following three prompts. Use the format given above but here there is no need to set the persona (i.e. only include one dictionary in the messages list when calling `client.chat.completions.create`.) Comment on any differences.\n",
    "\n",
    "1) \"Summarise the following article in 3 sentences.\"\n",
    "\n",
    "2) \"Give me a TL;DR of this text.\"\n",
    "\n",
    "3) \"What's the key takeaway here?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c372155-bdde-45a4-a184-fce4c4e8034c",
   "metadata": {
    "id": "9c372155-bdde-45a4-a184-fce4c4e8034c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's the key takeaway here?\n",
      "Response: A magnitude 4.2 earthquake struck the San Francisco area, causing about 2,000 power outages, but no injuries or significant damage were reported, with the quake being described as \"light\" and \"fairly mild\".\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Summarise the following article in exactly 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"] #update the prompt to 'exactly 3 sentences' as it was outputting 4 sentences.\n",
    "#content will be p + story for p in prompts\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": p + story\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "    \n",
    "print(f\"Prompt: {p}\")\n",
    "print(\"Response:\", response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df",
   "metadata": {
    "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df"
   },
   "source": [
    "Run the above code again below and note that the answers may differ. This is due to the probabilistic nature of LLM token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14",
   "metadata": {
    "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's the key takeaway here?\n",
      "Response: A 4.2 magnitude earthquake struck the San Francisco area, causing minimal damage and no reported injuries, with approximately 2,000 customers losing power.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": p + story\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "    \n",
    "print(f\"Prompt: {p}\")\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {
    "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f"
   },
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {
    "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf"
   },
   "source": [
    "**Exercise**: In this section adjust the `max_completion_tokens` and `temperature` settings below to obtain different responses. Show some examples with the prompt \"Continue the story: It was a great time to be alive\" with the model \"llama-3.1-8b-instant\".\n",
    "\n",
    "* max_completion_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens (set to 200 and 500)\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output (set to 0.2, 0.8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50207704-94be-4309-8a82-dc1d22a063ee",
   "metadata": {
    "id": "50207704-94be-4309-8a82-dc1d22a063ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's the key takeaway here?\n",
      "Response: The key takeaway is that a magnitude 4.2 earthquake occurred in the San Francisco area, causing disruptions such as power outages and numerous calls to authorities, but no reported injuries or significant damage due to the relatively mild intensity of the quake.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=200, do not have a temperature setting)\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": p + story\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    max_completion_tokens=200\n",
    ")\n",
    "\n",
    "    \n",
    "print(f\"Prompt: {p}\")\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63235fcc-3a61-4829-998b-37f2f104322b",
   "metadata": {
    "id": "63235fcc-3a61-4829-998b-37f2f104322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's the key takeaway here?\n",
      "Response: The key takeaway is that a magnitude 4.2 earthquake occurred in the San Francisco area at 4:42 a.m., resulting in minimal damage and no reported injuries. It caused a power outage for 2,000 customers and triggered concerns but overall was described as a minor, \"light\" quake that only lasted 50 seconds.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=500, do not have a temperature setting)\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": p + story\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    max_completion_tokens=500\n",
    ")\n",
    "\n",
    "    \n",
    "print(f\"Prompt: {p}\")\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {
    "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's the key takeaway here?\n",
      "Response: The key takeaway is that a magnitude 4.2 earthquake occurred in the San Francisco area, causing minimal damage and no reported injuries, but leaving about 2,000 customers without power and triggering alarm systems at homes.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 0.2, do not have a max_completion_tokens setting)\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": p + story\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "    \n",
    "print(f\"Prompt: {p}\")\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {
    "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's the key takeaway here?\n",
      "Response: The key takeaway is that a magnitude 4.2 earthquake occurred in the San Francisco area, causing minimal damage and no reported injuries. The quake, which lasted about 50 seconds, left around 2,000 customers without power, but overall it's considered a \"light\" earthquake with typical effects such as breaking dishes and overturning unstable objects.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 1, do not have a max_completion_tokens setting)\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": p + story\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "    \n",
    "print(f\"Prompt: {p}\")\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {
    "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf"
   },
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "931f8e00-928c-4218-8e74-8c56269fbfcb",
   "metadata": {
    "id": "931f8e00-928c-4218-8e74-8c56269fbfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's the key takeaway here?\n",
      "Response: A moderate earthquake (4.2 magnitude) in the San Francisco area caused mostly minor concerns, but minimal reported damage or injuries despite affecting around 2,000 PG&E power customers.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 2, do not have a max_completion_tokens setting)\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": p + story\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=2\n",
    ")\n",
    "\n",
    "    \n",
    "print(f\"Prompt: {p}\")\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {
    "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c"
   },
   "source": [
    "### Zero-shot and one-short prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {
    "id": "f6662f0a-c7e3-4235-972d-2771781a4e53"
   },
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {
    "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The reaction between two chemicals can be complex and depends on various factors, such as the type of chemicals involved, the conditions under which they are brought together, and the energy available. Here's a general overview of how two chemicals react:\n",
       "\n",
       "1. **Collision Theory**: For a chemical reaction to occur, the molecules of the two substances must collide with each other. The collision must be energetic enough to break the bonds between the atoms in the molecules and form new bonds.\n",
       "2. **Activation Energy**: The energy required for the molecules to collide and react is called the activation energy. If the energy is too low, the molecules will simply bounce off each other, and no reaction will occur.\n",
       "3. **Chemical Reaction Types**: There are several types of chemical reactions, including:\n",
       "\t* **Synthesis reaction**: Two molecules combine to form a new molecule (e.g., H2 + O2 â H2O).\n",
       "\t* **Decomposition reaction**: A single molecule breaks down into two or more molecules (e.g., 2H2O â 2H2 + O2).\n",
       "\t* **Replacement reaction**: One molecule replaces another molecule in a compound (e.g., Zn + CuSO4 â ZnSO4 + Cu).\n",
       "\t* **Combustion reaction**: A molecule reacts with oxygen to produce heat and light (e.g., CH4 + 2O2 â CO2 + 2H2O).\n",
       "4. **Reaction Mechanism**: The reaction mechanism is the sequence of steps that occurs during the reaction. It can be a single step or multiple steps, and it may involve the formation of intermediate molecules.\n",
       "5. **Factors Affecting Reaction Rate**: The rate of a chemical reaction can be influenced by various factors, including:\n",
       "\t* **Concentration**: Increasing the concentration of one or both reactants can increase the reaction rate.\n",
       "\t* **Temperature**: Increasing the temperature can increase the reaction rate, but it can also lead to unwanted side reactions.\n",
       "\t* **Catalysts**: Adding a catalyst can increase the reaction rate by lowering the activation energy.\n",
       "\t* **Surface area**: Increasing the surface area of one or both reactants can increase the reaction rate.\n",
       "\n",
       "To understand how two chemicals react, you need to know the chemical formulas and properties of the substances, as well as the conditions under which they are brought together. You can use various tools, such as:\n",
       "\n",
       "* **Chemical equation**: A chemical equation is a shorthand way of representing a chemical reaction.\n",
       "* **Balanced equation**: A balanced equation is a chemical equation in which the number of atoms of each element is the same on both the reactant and product sides.\n",
       "* **Reaction diagram**: A reaction diagram is a graphical representation of a chemical reaction, showing the reactants, products, and intermediate molecules.\n",
       "\n",
       "Here's an example of a simple chemical reaction:\n",
       "\n",
       "**Reaction:** H2 + O2 â H2O\n",
       "\n",
       "**Reactants:** Hydrogen gas (H2) and oxygen gas (O2)\n",
       "\n",
       "**Products:** Water (H2O)\n",
       "\n",
       "**Conditions:** The reaction occurs when the hydrogen and oxygen gases are brought together in the presence of a spark or heat.\n",
       "\n",
       "**Mechanism:** The reaction involves the collision of a hydrogen molecule with an oxygen molecule, resulting in the formation of a water molecule.\n",
       "\n",
       "**Factors affecting reaction rate:** The reaction rate can be affected by the concentration of the reactants, the temperature, and the presence of a catalyst.\n",
       "\n",
       "Keep in mind that this is a simplified example, and the actual reaction mechanism may be more complex."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {
    "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d"
   },
   "source": [
    "**Exercise:** Ask the same question but modify the prompt to return the answer to the same question in a simpler form (still using the llama-3.1-8b-instant model). Experiment with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {
    "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chemical reactions are like cooking recipes in a lab. Imagine you have different ingredients, like food or building blocks, that combine in specific ways to create something new.\n",
       "\n",
       "**Chemical Reaction Basics:**\n",
       "\n",
       "1. **Reactants**: These are the ingredients or substances that start the reaction.\n",
       "2. **Products**: These are the new substances created by combining the reactants.\n",
       "3. **Chemical Change**: This is when the reactants transform into products, changing their properties or appearance.\n",
       "\n",
       "**Types of Chemical Reactions:**\n",
       "\n",
       "1. **Combination Reaction**: This is like mixing ingredients to create something new. For example, making a cake by combining flour, sugar, eggs, and milk.\n",
       "2. **Decomposition Reaction**: This is like breaking down ingredients to create something simpler. For example, when you leave food out and it spoils, breaking down into simpler compounds.\n",
       "3. **Single Element Reaction**: This is like combining two elements, like hydrogen and oxygen (H2 + O2) to create water (H2O).\n",
       "4. **Displacement Reaction**: This is like replacing one element with another. For example, when iron (Fe) is added to copper oxide, it replaces the copper (Cu) to form iron oxide (Fe2O3).\n",
       "\n",
       "**Factors That Influence Reactions:**\n",
       "\n",
       "1. **Temperature**: Heat can speed up or slow down reactions. Think of a cake recipe that requires baking at 350Â°F.\n",
       "2. **Pressure**: Applying pressure can affect the rate of a reaction. Imagine squeezing a bottle of soda to create more pressure.\n",
       "3. **Light**: Some reactions require light energy, like photosynthesis in plants.\n",
       "4. **Catalysts**: These are substances that help speed up reactions without being used up. Think of a chef's favorite spice that enhances flavor.\n",
       "\n",
       "**Remember:**\n",
       "\n",
       "* Chemical reactions involve changes in the arrangement of particles, creating new substances.\n",
       "* Factors like temperature, pressure, and light can influence reactions.\n",
       "* Understanding chemical reactions can help us create new products, like medicines, fuels, and food.\n",
       "\n",
       "Now, you have a basic understanding of chemical reactions. It's like cooking, but with atoms and molecules instead of ingredients!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do chemicals react? Explain this using easy to understand lament terms.\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {
    "id": "04676a11-57a7-4f9c-87e2-128559f0ffce"
   },
   "source": [
    "### One-shot prompting ###\n",
    "\n",
    "Next, note the dramatic change when we give the following template setting a new role and providing an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {
    "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment deux composÃ©s chimiques rÃ©agissent-ils ?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"system\",\n",
    "             \"content\": \"You translate English to French.\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What time is it?\"},\n",
    "               {\"role\": \"assistant\",\n",
    "               \"content\": \"Quelle heure est-il?\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {
    "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269"
   },
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {
    "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050"
   },
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. This is where examples can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b",
   "metadata": {
    "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I regret to inform you that I will be unable to attend the meeting.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"I'm gonna head out now, see you later.\"\n",
    "response1 = \"I will be leaving now. See you later.\"\n",
    "\n",
    "prompt2 =  \"That movie was super cool!\"\n",
    "response2 = \"The movie was very impressive.\"\n",
    "\n",
    "prompt3 = \"Can't make it to the meeting, sorry.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional editor. Rewrite casual sentences into a formal tone.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e9e18-cc27-453b-8844-3f683fb607f8",
   "metadata": {
    "id": "040e9e18-cc27-453b-8844-3f683fb607f8"
   },
   "source": [
    "The output can also be moulded to provide SQL output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {
    "id": "ed854534-30db-4745-b910-27d12a3ed47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM products WHERE quantity_in_stock = 0;\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Show me all users who signed up in the last 30 days.\"\n",
    "response1 = \"SELECT * FROM users WHERE signup_date >= CURRENT_DATE - INTERVAL '30 days';\"\n",
    "\n",
    "prompt2 = \"What is the average order value?\"\n",
    "response2 =  \"SELECT AVG(order_total) FROM orders;\"\n",
    "\n",
    "prompt3 = \"List products that are out of stock.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that translates natural language to SQL.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7",
   "metadata": {
    "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7"
   },
   "source": [
    "**Exercise**: Create a few examples to train the \"llama3-70b-8192\" LLM to take in user content in the form below and provide output as a pandas dataframe. Use the `exec` function to execute its output to display the answer of sample input as a data frame.\n",
    "\n",
    "Example:\n",
    "\n",
    "given the user content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 32 | 27 | 25\n",
    "\n",
    "| 64 | 23 | 14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train the model to output\n",
    "\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b8ac08a-5759-41cd-a560-e77694573723",
   "metadata": {
    "id": "2b8ac08a-5759-41cd-a560-e77694573723"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carol</td>\n",
       "      <td>22</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age  Score\n",
       "0  Alice   25     88\n",
       "1    Bob   30     92\n",
       "2  Carol   22     95"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = '''\n",
    "| col1 | col2 | col3\n",
    "| 32   | 27   | 25\n",
    "| 64   | 23   | 14\n",
    "'''\n",
    "\n",
    "output1 = '''\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "'''\n",
    "example2 = '''\n",
    "| A | B    | C\n",
    "| 4 | 2333 | 78\n",
    "| 2 | 222  | 65\n",
    "'''\n",
    "\n",
    "output2 = '''\n",
    "df = pd.DataFrame({'A': [4, 2], 'B': [2333, 222], 'C': [78, 65]})\n",
    "'''\n",
    "\n",
    "example3 = \"\"\"\n",
    "| Name   | Age | Score\n",
    "| Alice  | 25  | 88\n",
    "| Bob    | 30  | 92\n",
    "| Carol  | 22  | 95\n",
    "\"\"\"\n",
    "\n",
    "output3 = '''\n",
    "df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Carol'], 'Age': [25, 30, 22], 'Score': [88, 92, 95]})\n",
    "'''\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Use the examples to create a dataframe identified as df and to execute the output\"},\n",
    "        {\"role\": \"user\", \"content\": example1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": example2},\n",
    "        {\"role\": \"assistant\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": example3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "exec(response.choices[0].message.content.strip()) # string executed as Python code\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f62a88-c7b4-4431-acda-97e9a98981f4",
   "metadata": {
    "id": "29f62a88-c7b4-4431-acda-97e9a98981f4"
   },
   "source": [
    "Also show what happens when the question is asked in the absence of a system role and without few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6",
   "metadata": {
    "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A nice little table!\\n\\nIt looks like we have a table with three columns: `Name`, `Age`, and `Score`. And three rows of data, each representing a person.\\n\\nHere's a quick summary of the data:\\n\\n* The names are Alice, Bob, and Carol.\\n* Their ages are 25, 30, and 22, respectively.\\n* Their scores are 88, 92, and 95, respectively.\\n\\nIs there anything specific you'd like to do with this data or ask about it?\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": example3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content.strip() # string executed as Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {
    "id": "28660884-dfd4-4b96-a65d-d21ddbf99423"
   },
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {
    "id": "724f85ef-afc0-4e31-9ad8-ae780c535837"
   },
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee04ad6-968b-406a-8119-981b01ef5f92",
   "metadata": {
    "id": "bee04ad6-968b-406a-8119-981b01ef5f92"
   },
   "source": [
    "**Exercise**: Using the following prompts, compare the answers of the \"llama3-8b-8192\" model (set seed=21). (If this model is no longer available choose a model with relatively few parameters.)\n",
    "\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb",
   "metadata": {
    "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot:\n",
      " There are 2 s's in the word \"success\". \n",
      "\n",
      "Chain of thought:\n",
      " Let's go through each letter of the word \"success\" to count the number of s's:\n",
      "\n",
      "1. S (first letter)\n",
      "2. U\n",
      "3. C\n",
      "4. C\n",
      "5. E\n",
      "6. S\n",
      "7. S\n",
      "\n",
      "As we go through the letters, we find two S's (at positions 1 and 6) and three other letters. Therefore, the correct answer is:\n",
      "\n",
      "There are 2 S's in the word \"success\".\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    seed=21,\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": zero_shot_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    seed=21,\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": chain_of_thought_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('Zero shot:\\n',response1.choices[0].message.content,'\\n')\n",
    "print('Chain of thought:\\n', response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f",
   "metadata": {
    "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f"
   },
   "source": [
    "## Comparison of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f",
   "metadata": {
    "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f"
   },
   "source": [
    "**Exercise**: Compare the performance of 2 LLMs by outputting the answers of the following questions into a dataframe.\n",
    "\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "\n",
    "Column headings:\n",
    "\n",
    "Model Name | Question | Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238",
   "metadata": {
    "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # allows wide dataframes to be viewed\n",
    "models = models = {\n",
    "    \"gemma2-9b-it\": \"Gemma2 9b\",\n",
    "    \"llama3-8b-8192\": \"LLaMA 3 8B\"\n",
    "} #can edit this\n",
    "\n",
    "# ANSWER\n",
    "prompts = [\"Tell me a joke about data science.\",\n",
    "\"How can one calculate 22 * 13 mentally?\",\n",
    "\"Write a creative story about a baby learning to crawl.\",]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_id, model_name in models.items():\n",
    "    for prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        results.append({\n",
    "            \"Model Name\": model_name,\n",
    "            \"Question\": prompt,\n",
    "            \"Answer\": answer\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bdf7d2f-43aa-42eb-9ebb-149f79935538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemma2 9b</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist break up with the statistician? \\n\\nBecause they had too many standard deviations! ð  \\n\\n\\nLet me know if you'd like to hear another one! ð</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gemma2 9b</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>Here's a way to calculate 22 * 13 mentally:\\n\\n**Break it down:**\\n\\n* **Think of 22 as (20 + 2)**\\n* **Use the distributive property:** \\n   * 22 * 13 = (20 + 2) * 13 \\n   * = 20 * 13 + 2 * 13\\n\\n* **Calculate the simpler parts:**\\n   * 20 * 13 = 260\\n   * 2 * 13 = 26\\n\\n* **Add the results:**\\n   * 260 + 26 = 286\\n\\n\\n**Therefore, 22 * 13 = 286**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemma2 9b</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Bartholomew \"Barry\" Butterfield III, a chubby, cherub-faced baby with a shock of unruly ginger hair, stared intently at the magnificent, towering structure that was his mother's favorite houseplant. It was a fern, a leafy green behemoth that reached almost to the coffee table, its fronds unfurling like emerald flags in the afternoon sun. \\n\\nBarry coveted the fern.  Not for its exotic greenery, but for its precarious position atop a wobbly toy box. Every morning, his mother would perch the box on the rug, tempting him with the forbidden land of soft-plush toys and crinkly treat bags. And  just beyond that tempting space, the fern. It felt like a magical kingdom just out of reach. \\n\\nFor weeks, Barry had practiced pushing and scooting himself with his chubby arms. He'd propel himself backward, forwards, and sometimes even sideways, but never onward to the glorious fern. He'd watch his older sister, Penelope, glide across the room with the ease of a newborn gazelle, and scoff.  Crawling was for show! He would conquer it on his own terms, in a feat of unparalleled baby brilliance. \\n\\nOne sunny morning, a determined glint ignited in Barry's baby blue eyes. He positioned himself, feet planted firmly, then, with a grunt and a wiggle, thrust his arms forward. His face scrunched in concentration, his bottom lifting and lowering like a mini marshmallow on a hot pan. \\n\\nHe moved!  It was a jerky, stuttering dance, his arms flailing wildly, his legs splaying like a confused spider. But he was moving! He was creeping towards his coveted target!\\n\\nHis mother, who had been diligently working on a cross-stitch project, watched in amusement. \\n\\nâThatâs it, Barry! Youâre doing it!â \\n\\nHer cheery encouragement only fueled Barryâs determination. He forgot his wobbly gait, his awkward limbs, and his mother's increasingly bewildered narration of his journey.  He was on a mission! His tiny brow furrowed, his tongue poked out in concentration.\\n\\nFinally, after what seemed like an eternity (to Barry, at least), he reached the toy box. His chest heaved, his tiny heart pounded with triumph. He had conquered the first obstacle!\\n\\nBut the fern still loomed, a verdant promise just beyond. With renewed vigor, Barry pulled himself toward the leafy giant, his legs churning, his arms reaching, his face a mask of sweaty, triumphant determination.\\n\\nAt last, he reached the fernâs base. He leaned forward, his chubby hand reaching out to touch the delicate fronds. He had done it! He had climbed Mount Fern! \\n\\nBarryâs mother, tears welling up in her eyes, scooped him into her arms.\\n\\n\"My brave little explorer!\" she exclaimed.\\n\\nBarry, exhausted but proud, gurgled happily, nibbling at his motherâs cheek.  The world felt full of adventures, and there were so many more ferns to conquer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLaMA 3 8B</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist quit his job?\\n\\n porque he didn't get the correlation he was looking for!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLaMA 3 8B</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>There are several ways to calculate 22 * 13 mentally, and here are a few:\\n\\n**Method 1: Breaking down the calculation**\\nBreak down the calculation into two parts: 20 * 13 and 2 * 13. Then add the results:\\n20 * 13 = 260 (quickly multiply 20 by 10 = 200, then add 3 x 20 = 60)\\n2 * 13 = 26\\nAdd both results: 260 + 26 = 286\\n\\n**Method 2: Using multiplication facts**\\nUse your multiplication facts to calculate:\\n22 * 10 = 220 (common fact)\\n220 * 1 = 220 (since 13 is 10 + 3, you can use this fact)\\nAdd 2 * 13 = 26 (quickly multiply 2 by 13: 2 x 10 = 20, then add 3 x 2 = 6)\\nAdd both results: 220 + 26 = 246 (no, wait, 286!)\\n\\n**Method 3: Using doubling and doubling**\\nDouble 22 to get 44, then double it again to get 88. Now, subtract 44 from 88 to get 44, which is half of 22 * 13.\\n44 * 2 = 88 (quickly calculate)\\n88 - 44 = 44\\n\\n**Method 4: Using a multiplication table**\\nIf you have a multiplication table memorized, you can quickly look up the result: 22 * 13 = 286.\\n\\nThese are just a few methods to calculate 22 * 13 mentally. Practice makes perfect, so try out different methods to see which one works best for you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLaMA 3 8B</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Once upon a time, in a cozy little nest of soft blankets and plush toys, a baby named Lily lay sprawled out on the floor. She had been watching the world go by through her big, curious eyes for a while now, and she was determined to be a part of it. Or at least, to get to the toys that were just out of reach.\\n\\nLily's arms and legs squirmed and kicked, trying to propel herself towards the nearest toy, a bright red ball that rolled away whenever she tried to grab it. Her chubby little hands waved in the air, as if trying to catch a bird in flight.\\n\\nJust then, a gentle breeze rustled the leaves of a nearby plant, causing the plant to sway slightly. Lily's eyes followed the motion, and suddenly, an idea popped into her head. Why not try to move her body in the same way? She pushed off with her legs, and to her surprise, she started to inch forward, her arms wiggling back and forth like a little worm.\\n\\nEureka! Lily had crawled! Well, probably not in the classical sense, but she had definitely moved her body in a way that brought her closer to that tantalizing red ball. And wasn't that what mattered?\\n\\nAs the days went by, Lily grew more confident in her newfound skill. She would crawl, stop, and then reverse direction, using her arms to steer herself like a little tugboat. Her parents cheered and clapped whenever she made progress, and soon she was zooming across the floor, her little legs pumping furiously.\\n\\nOne day, Lily spotted a particularly enticing toy just out of reach. She fixed her eyes on it, took a deep breath, and launched herself in the direction of her heart's desire. This time, she didn't just inch forward â she crawled, fast and furious, her whole body moving like a machine.\\n\\nAnd then, in a flash of triumph, her little hand closed around the toy, and she beamed with pride. \"I did it!\" she squealed, holding the toy aloft like a trophy.\\n\\nFrom that day on, Lily was unstoppable. She crawled through tunnel after tunnel, over pillows and under blankets, her giggles echoing through the air. And her parents, watching with pride, knew that this little ball of energy was going to grow up to be a brave adventurer, always reaching for the next great challenge.\\n\\nThe end.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name                                                Question  \\\n",
       "0   Gemma2 9b                      Tell me a joke about data science.   \n",
       "1   Gemma2 9b                 How can one calculate 22 * 13 mentally?   \n",
       "2   Gemma2 9b  Write a creative story about a baby learning to crawl.   \n",
       "3  LLaMA 3 8B                      Tell me a joke about data science.   \n",
       "4  LLaMA 3 8B                 How can one calculate 22 * 13 mentally?   \n",
       "5  LLaMA 3 8B  Write a creative story about a baby learning to crawl.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Why did the data scientist break up with the statistician? \\n\\nBecause they had too many standard deviations! ð  \\n\\n\\nLet me know if you'd like to hear another one! ð  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Here's a way to calculate 22 * 13 mentally:\\n\\n**Break it down:**\\n\\n* **Think of 22 as (20 + 2)**\\n* **Use the distributive property:** \\n   * 22 * 13 = (20 + 2) * 13 \\n   * = 20 * 13 + 2 * 13\\n\\n* **Calculate the simpler parts:**\\n   * 20 * 13 = 260\\n   * 2 * 13 = 26\\n\\n* **Add the results:**\\n   * 260 + 26 = 286\\n\\n\\n**Therefore, 22 * 13 = 286**  \n",
       "2  Bartholomew \"Barry\" Butterfield III, a chubby, cherub-faced baby with a shock of unruly ginger hair, stared intently at the magnificent, towering structure that was his mother's favorite houseplant. It was a fern, a leafy green behemoth that reached almost to the coffee table, its fronds unfurling like emerald flags in the afternoon sun. \\n\\nBarry coveted the fern.  Not for its exotic greenery, but for its precarious position atop a wobbly toy box. Every morning, his mother would perch the box on the rug, tempting him with the forbidden land of soft-plush toys and crinkly treat bags. And  just beyond that tempting space, the fern. It felt like a magical kingdom just out of reach. \\n\\nFor weeks, Barry had practiced pushing and scooting himself with his chubby arms. He'd propel himself backward, forwards, and sometimes even sideways, but never onward to the glorious fern. He'd watch his older sister, Penelope, glide across the room with the ease of a newborn gazelle, and scoff.  Crawling was for show! He would conquer it on his own terms, in a feat of unparalleled baby brilliance. \\n\\nOne sunny morning, a determined glint ignited in Barry's baby blue eyes. He positioned himself, feet planted firmly, then, with a grunt and a wiggle, thrust his arms forward. His face scrunched in concentration, his bottom lifting and lowering like a mini marshmallow on a hot pan. \\n\\nHe moved!  It was a jerky, stuttering dance, his arms flailing wildly, his legs splaying like a confused spider. But he was moving! He was creeping towards his coveted target!\\n\\nHis mother, who had been diligently working on a cross-stitch project, watched in amusement. \\n\\nâThatâs it, Barry! Youâre doing it!â \\n\\nHer cheery encouragement only fueled Barryâs determination. He forgot his wobbly gait, his awkward limbs, and his mother's increasingly bewildered narration of his journey.  He was on a mission! His tiny brow furrowed, his tongue poked out in concentration.\\n\\nFinally, after what seemed like an eternity (to Barry, at least), he reached the toy box. His chest heaved, his tiny heart pounded with triumph. He had conquered the first obstacle!\\n\\nBut the fern still loomed, a verdant promise just beyond. With renewed vigor, Barry pulled himself toward the leafy giant, his legs churning, his arms reaching, his face a mask of sweaty, triumphant determination.\\n\\nAt last, he reached the fernâs base. He leaned forward, his chubby hand reaching out to touch the delicate fronds. He had done it! He had climbed Mount Fern! \\n\\nBarryâs mother, tears welling up in her eyes, scooped him into her arms.\\n\\n\"My brave little explorer!\" she exclaimed.\\n\\nBarry, exhausted but proud, gurgled happily, nibbling at his motherâs cheek.  The world felt full of adventures, and there were so many more ferns to conquer.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Why did the data scientist quit his job?\\n\\n porque he didn't get the correlation he was looking for!  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            There are several ways to calculate 22 * 13 mentally, and here are a few:\\n\\n**Method 1: Breaking down the calculation**\\nBreak down the calculation into two parts: 20 * 13 and 2 * 13. Then add the results:\\n20 * 13 = 260 (quickly multiply 20 by 10 = 200, then add 3 x 20 = 60)\\n2 * 13 = 26\\nAdd both results: 260 + 26 = 286\\n\\n**Method 2: Using multiplication facts**\\nUse your multiplication facts to calculate:\\n22 * 10 = 220 (common fact)\\n220 * 1 = 220 (since 13 is 10 + 3, you can use this fact)\\nAdd 2 * 13 = 26 (quickly multiply 2 by 13: 2 x 10 = 20, then add 3 x 2 = 6)\\nAdd both results: 220 + 26 = 246 (no, wait, 286!)\\n\\n**Method 3: Using doubling and doubling**\\nDouble 22 to get 44, then double it again to get 88. Now, subtract 44 from 88 to get 44, which is half of 22 * 13.\\n44 * 2 = 88 (quickly calculate)\\n88 - 44 = 44\\n\\n**Method 4: Using a multiplication table**\\nIf you have a multiplication table memorized, you can quickly look up the result: 22 * 13 = 286.\\n\\nThese are just a few methods to calculate 22 * 13 mentally. Practice makes perfect, so try out different methods to see which one works best for you!  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Once upon a time, in a cozy little nest of soft blankets and plush toys, a baby named Lily lay sprawled out on the floor. She had been watching the world go by through her big, curious eyes for a while now, and she was determined to be a part of it. Or at least, to get to the toys that were just out of reach.\\n\\nLily's arms and legs squirmed and kicked, trying to propel herself towards the nearest toy, a bright red ball that rolled away whenever she tried to grab it. Her chubby little hands waved in the air, as if trying to catch a bird in flight.\\n\\nJust then, a gentle breeze rustled the leaves of a nearby plant, causing the plant to sway slightly. Lily's eyes followed the motion, and suddenly, an idea popped into her head. Why not try to move her body in the same way? She pushed off with her legs, and to her surprise, she started to inch forward, her arms wiggling back and forth like a little worm.\\n\\nEureka! Lily had crawled! Well, probably not in the classical sense, but she had definitely moved her body in a way that brought her closer to that tantalizing red ball. And wasn't that what mattered?\\n\\nAs the days went by, Lily grew more confident in her newfound skill. She would crawl, stop, and then reverse direction, using her arms to steer herself like a little tugboat. Her parents cheered and clapped whenever she made progress, and soon she was zooming across the floor, her little legs pumping furiously.\\n\\nOne day, Lily spotted a particularly enticing toy just out of reach. She fixed her eyes on it, took a deep breath, and launched herself in the direction of her heart's desire. This time, she didn't just inch forward â she crawled, fast and furious, her whole body moving like a machine.\\n\\nAnd then, in a flash of triumph, her little hand closed around the toy, and she beamed with pride. \"I did it!\" she squealed, holding the toy aloft like a trophy.\\n\\nFrom that day on, Lily was unstoppable. She crawled through tunnel after tunnel, over pillows and under blankets, her giggles echoing through the air. And her parents, watching with pride, knew that this little ball of energy was going to grow up to be a brave adventurer, always reaching for the next great challenge.\\n\\nThe end.  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5",
   "metadata": {
    "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5"
   },
   "source": [
    "### Bonus\n",
    "\n",
    "See if you can prompt an LLM to perform sentiment analysis (output 'Positive' or 'Negative' only) on a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "188c1a84-affc-4f50-8e66-96111b48f02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": \"Is this text Positive or Negative? Answer only with 'Positive' or 'Negative'.\\nText: I love this product!\\nSentiment:\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a",
   "metadata": {
    "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love how easy this product is to use!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The service was terrible and the staff were rude.\n",
      "Sentiment: Unknown\n",
      "\n",
      "Text: This coding is real hard.\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_sentiment(text: str, model_id=\"llama3-8b-8192\") -> str:\n",
    "    prompt = (\n",
    "        f\"Classify the sentiment of the following text as either 'Positive' or 'Negative' only.\\n\\n\"\n",
    "        f\"Text: \\\"{text}\\\"\\n\\n\"\n",
    "        f\"Sentiment:\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "    sentiment = response.choices[0].message.content.strip()\n",
    "    # Optional: ensure output is exactly 'Positive' or 'Negative'\n",
    "    if sentiment.lower() in ['positive', 'negative']:\n",
    "        return sentiment.capitalize()\n",
    "    else:\n",
    "        # fallback or retry could go here\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Example usage\n",
    "text_samples = [\n",
    "    \"I love how easy this product is to use!\",\n",
    "    \"The service was terrible and the staff were rude.\",\n",
    "    \"This coding is real hard.\",\n",
    "]\n",
    "\n",
    "for text in text_samples:\n",
    "    sentiment = get_sentiment(text)\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {
    "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {
    "id": "f279421d-cbca-4003-941e-c2d2bc2833a8"
   },
   "source": [
    "We worked with a few Large Language Models (LLMs) using Groq and experimented with prompting for summarisation, text completion and question-answering tasks.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting to achieve desired forms of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {
    "id": "94f06d5e-7073-485b-b046-afe839ab844c"
   },
   "source": [
    "## References\n",
    "1. [Groq's prompting guide](https://console.groq.com/docs/prompting)\n",
    "2. [Groq's playground](https://console.groq.com/playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {
    "id": "d61aab4a-1330-4762-9318-5635c3a97aa7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
